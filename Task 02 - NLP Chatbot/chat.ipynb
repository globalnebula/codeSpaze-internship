{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages\n",
    "- To install necessary python modules for NLP chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (1.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kunal\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary NLTK data\n",
    "- downloading punctuations, stopword, and wordnet to increase the accuracy of the chatbot and avoid training of stop words and punctuations like `.` and `,` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the spacy model for english\n",
    "- Load spaCy model using `spacy.load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset\n",
    "- Here we first create 2 empty arrays, `questions` and `answers`.\n",
    "- We know that out data is in the format `Consists of two columns: question \\t answer \\n . Suitable for simple chatbots. Contains 3725 items`.\n",
    "- The dataset is uploaded along with the `.ipynb` file or you can direct use the kaggle link [here](https://www.kaggle.com/datasets/grafstor/simple-dialogs-for-chatbot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            q, a = line.strip().split('\\t')\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = load_dataset('data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text\n",
    "- Tokenize\n",
    "- Remove stopwords and punctuation\n",
    "- Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess questions\n",
    "- Create TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "processed_questions = [preprocess(q) for q in questions]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "question_vectors = vectorizer.fit_transform(processed_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the most similar question\n",
    "- We first preprocess the user input.\n",
    "- Transform the input vector into `TF-IDF` vectors.\n",
    "- We find the similarties array using the `cosine_similarity` function between the `input_vectors` and `question_vectors`.\n",
    "- We find the similarity index using the `argmax` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_question(user_input):\n",
    "    processed_input = preprocess(user_input)\n",
    "    input_vector = vectorizer.transform([processed_input])\n",
    "    similarities = cosine_similarity(input_vector, question_vectors)\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    return most_similar_idx, similarities[0][most_similar_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate response using spaCy\n",
    "- First we set a threshold to similarity (`0.5`) which defines wether the chatbot answers the questioon or not.\n",
    "- Use spaCy for advanced NLP processing.\n",
    "- Check for negation in the document.\n",
    "- Identify the question type asked by the user.\n",
    "- Modify response based on NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    idx, similarity = get_most_similar_question(user_input)\n",
    "    \n",
    "    if similarity < 0.5: \n",
    "        return \"I'm sorry, I don't understand. Could you please rephrase your question?\"\n",
    "    \n",
    "    doc = nlp(user_input)\n",
    "    \n",
    "    negation = any(token.dep_ == 'neg' for token in doc)\n",
    "    \n",
    "    question_words = ['what', 'when', 'where', 'who', 'why', 'how']\n",
    "    question_type = next((token.text.lower() for token in doc if token.text.lower() in question_words), None)\n",
    "    \n",
    "    response = answers[idx]\n",
    "    if negation:\n",
    "        response = \"I understand you're asking a negative question. \" + response\n",
    "    if question_type:\n",
    "        response = f\"To answer your {question_type} question: \" + response\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I help you today? (Type 'quit' to exit)\n",
      "User: Hi, how are you\n",
      "Chatbot: To answer your how question: i'm fine. how about yourself?\n",
      "User: hiiii then\n",
      "Chatbot: I'm sorry, I don't understand. Could you please rephrase your question?\n",
      "User: what do you do in your free time\n",
      "Chatbot: To answer your what question: it starts at 8 o'clock.\n",
      "User: which school do you attend\n",
      "Chatbot: i'm attending Amrita Vishwa Vidyapeetham right now.\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot: Hello! How can I help you today? (Type 'quit' to exit)\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = generate_response(user_input)\n",
    "    print(\"User:\", user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(test_size=0.2, random_state=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    train_questions, test_questions, train_answers, test_answers = train_test_split(\n",
    "        questions, answers, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Preprocess and vectorize the training questions\n",
    "    processed_train_questions = [preprocess(q) for q in train_questions]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_question_vectors = vectorizer.fit_transform(processed_train_questions)\n",
    "    \n",
    "    # Initialize counters\n",
    "    correct = 0\n",
    "    total = len(test_questions)\n",
    "    \n",
    "    for test_q, test_a in zip(test_questions, test_answers):\n",
    "        # Get the model's response\n",
    "        response = generate_response(test_q)\n",
    "        \n",
    "        # Compare the response to the actual answer\n",
    "        if response.lower() == test_a.lower():\n",
    "            correct += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate_model()\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
